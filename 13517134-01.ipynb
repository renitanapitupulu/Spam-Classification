{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nama : Renita Napitupulu\n",
    "<br>\n",
    "NIM  : 13517134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", 'wait', 'to', 'watch', 'the', 'next', 'episode']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Tokenization using nltk\n",
    "sentence = \"I can't wait to watch the next episode\"\n",
    "\n",
    "tokens =  nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('ca', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('wait', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('watch', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('next', 'JJ'),\n",
       " ('episode', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. POS Tagger using nltk\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time fly so fast'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Lemmatization using nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"time flies so fast\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "after_lemmatized = ' '.join([lemmatizer.lemmatize(t) for t in tokens])\n",
    "after_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time fli so fast'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Stemming using nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "after_stemming = ' '.join([ps.stem(t) for t in tokens])\n",
    "after_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She doe n't know what to do\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Lemmatization using nltk\n",
    "sentence = \"She doesn't know what to do\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "after_lemmatized = ' '.join([lemmatizer.lemmatize(t) for t in tokens])\n",
    "after_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp =  spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "ca\n",
      "n't\n",
      "wait\n",
      "to\n",
      "watch\n",
      "the\n",
      "next\n",
      "episode\n"
     ]
    }
   ],
   "source": [
    "# 6. Tokenization using spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "sentence = \"I can't wait to watch the next episode\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "ca VERB\n",
      "n't PART\n",
      "wait VERB\n",
      "to PART\n",
      "watch VERB\n",
      "the DET\n",
      "next ADJ\n",
      "episode NOUN\n"
     ]
    }
   ],
   "source": [
    "# 7. POS Tagger using spacy\n",
    "sentence = \"I can't wait to watch the next episode\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time fly so fast'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Lemmatization using spacy\n",
    "sentence = \"time flies so fast\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "after_lemmatized = ' '.join([token.lemma_ for token in doc])\n",
    "after_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> nsubj\n",
      "ca --> aux\n",
      "n't --> neg\n",
      "wait --> ROOT\n",
      "to --> aux\n",
      "watch --> xcomp\n",
      "the --> det\n",
      "next --> amod\n",
      "episode --> dobj\n"
     ]
    }
   ],
   "source": [
    "# 9. Dependency Parsing using spacy\n",
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesia --> GPE\n",
      "Asian --> NORP\n"
     ]
    }
   ],
   "source": [
    "# 10. Named Entity Recognition using spacy\n",
    "sentence = \"Indonesia will host the upcoming Asian Games 2024\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-->\", ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
